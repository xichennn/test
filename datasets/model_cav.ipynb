{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/carla_data/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from itertools import permutations\n",
    "from itertools import product\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/Users/xichen/Documents/paper2-traj-pred/carla-data')\n",
    "import VectorNet.utils.carla_process_cav as preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../../scene_mining_cav/\"\n",
    "dataset_input_path = os.path.join(data_root)\n",
    "train_dataset = preprocess.scene_processed_dataset(dataset_input_path, 'train', map_radius=75)\n",
    "val_dataset = preprocess.scene_processed_dataset(dataset_input_path, 'val', map_radius=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scene_processed_dataset(45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset)\n",
    "val_loader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args\n",
    "historical_steps = 50\n",
    "future_steps = 50\n",
    "num_modes = 6\n",
    "node_dim = 2\n",
    "edge_dim = 2\n",
    "embed_dim = 64\n",
    "rotate = True\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "num_temporal_layers = 4\n",
    "num_global_layers = 3\n",
    "local_radius = 75\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-4\n",
    "T_max = 64\n",
    "parallel = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HiVT.utils import init_weights\n",
    "\n",
    "\n",
    "class SingleInputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channel: int,\n",
    "                 out_channel: int) -> None:\n",
    "        super(SingleInputEmbedding, self).__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(in_channel, out_channel),\n",
    "            nn.LayerNorm(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_channel, out_channel),\n",
    "            nn.LayerNorm(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_channel, out_channel),\n",
    "            nn.LayerNorm(out_channel))\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class MultipleInputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: List[int],\n",
    "                 out_channel: int) -> None:\n",
    "        super(MultipleInputEmbedding, self).__init__()\n",
    "        self.module_list = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Linear(in_channel, out_channel),\n",
    "                           nn.LayerNorm(out_channel),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.Linear(out_channel, out_channel))\n",
    "             for in_channel in in_channels])\n",
    "        self.aggr_embed = nn.Sequential(\n",
    "            nn.LayerNorm(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_channel, out_channel),\n",
    "            nn.LayerNorm(out_channel))\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                continuous_inputs: List[torch.Tensor],\n",
    "                categorical_inputs: Optional[List[torch.Tensor]] = None) -> torch.Tensor:\n",
    "        for i in range(len(self.module_list)):\n",
    "            continuous_inputs[i] = self.module_list[i](continuous_inputs[i])\n",
    "        output = torch.stack(continuous_inputs).sum(dim=0)\n",
    "        if categorical_inputs is not None:\n",
    "            output += torch.stack(categorical_inputs).sum(dim=0)\n",
    "        return self.aggr_embed(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022, Zikang Zhou. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# sys.path.append('/Users/xichen/Documents/paper2-traj-pred/carla-data/HiVT')\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.typing import Size\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "# from HiVT.models.embedding import MultipleInputEmbedding\n",
    "# from HiVT.models.embedding import SingleInputEmbedding\n",
    "from HiVT.utils import DistanceDropEdge\n",
    "from HiVT.utils import TemporalData\n",
    "from HiVT.utils import init_weights\n",
    "\n",
    "\n",
    "class LocalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 historical_steps: int,\n",
    "                 node_dim: int,\n",
    "                 edge_dim: int,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 dropout: float = 0.1,\n",
    "                 num_temporal_layers: int = 4,\n",
    "                 local_radius: float = 50,\n",
    "                 parallel: bool = False) -> None:\n",
    "        super(LocalEncoder, self).__init__()\n",
    "        self.historical_steps = historical_steps\n",
    "        self.parallel = parallel\n",
    "\n",
    "        self.drop_edge = DistanceDropEdge(local_radius)\n",
    "        self.aa_encoder = AAEncoder(historical_steps=historical_steps,\n",
    "                                    node_dim=node_dim,\n",
    "                                    edge_dim=edge_dim,\n",
    "                                    embed_dim=embed_dim,\n",
    "                                    num_heads=num_heads,\n",
    "                                    dropout=dropout,\n",
    "                                    parallel=parallel)\n",
    "        self.temporal_encoder = TemporalEncoder(historical_steps=historical_steps,\n",
    "                                                embed_dim=embed_dim,\n",
    "                                                num_heads=num_heads,\n",
    "                                                dropout=dropout,\n",
    "                                                num_layers=num_temporal_layers)\n",
    "        self.al_encoder = ALEncoder(node_dim=node_dim,\n",
    "                                    edge_dim=edge_dim,\n",
    "                                    embed_dim=embed_dim,\n",
    "                                    num_heads=num_heads,\n",
    "                                    dropout=dropout)\n",
    "\n",
    "    def forward(self, data: TemporalData) -> torch.Tensor:\n",
    "        for t in range(self.historical_steps):\n",
    "            data[f'edge_index_{t}'], _ = subgraph(subset=~data['padding_mask'][:, t], edge_index=data.edge_index)\n",
    "            data[f'edge_attr_{t}'] = \\\n",
    "                data['positions'][data[f'edge_index_{t}'][0], t] - data['positions'][data[f'edge_index_{t}'][1], t]\n",
    "        if self.parallel:\n",
    "            snapshots = [None] * self.historical_steps\n",
    "            for t in range(self.historical_steps):\n",
    "                edge_index, edge_attr = self.drop_edge(data[f'edge_index_{t}'], data[f'edge_attr_{t}'])\n",
    "                snapshots[t] = Data(x=data.x[:, t], edge_index=edge_index, edge_attr=edge_attr,\n",
    "                                    num_nodes=data.num_nodes)\n",
    "            batch = Batch.from_data_list(snapshots)\n",
    "            out = self.aa_encoder(x=batch.x, t=None, edge_index=batch.edge_index, edge_attr=batch.edge_attr,\n",
    "                                  bos_mask=data['bos_mask'], rotate_mat=data['rotate_mat'])\n",
    "            out = out.view(self.historical_steps, out.shape[0] // self.historical_steps, -1)\n",
    "        else:\n",
    "            out = [None] * self.historical_steps\n",
    "            for t in range(self.historical_steps):\n",
    "                edge_index, edge_attr = self.drop_edge(data[f'edge_index_{t}'], data[f'edge_attr_{t}'])\n",
    "                out[t] = self.aa_encoder(x=data.x[:, t], t=t, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                                         bos_mask=data['bos_mask'][:, t], rotate_mat=data['rotate_mat'])\n",
    "            out = torch.stack(out)  # [T, N, D]\n",
    "        out = self.temporal_encoder(x=out, padding_mask=data['padding_mask'][:, : self.historical_steps])\n",
    "        edge_index, edge_attr = self.drop_edge(data['lane_actor_index'], data['lane_actor_vectors'])\n",
    "        out = self.al_encoder(x=(data['lane_vectors'], out), edge_index=edge_index, edge_attr=edge_attr,\n",
    "                              is_intersections=data['is_intersections'], turn_directions=data['turn_directions'],\n",
    "                              traffic_controls=data['traffic_controls'], rotate_mat=data['rotate_mat'])\n",
    "        return out\n",
    "\n",
    "\n",
    "class AAEncoder(MessagePassing):\n",
    "\n",
    "    def __init__(self,\n",
    "                 historical_steps: int,\n",
    "                 node_dim: int,\n",
    "                 edge_dim: int,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 dropout: float = 0.1,\n",
    "                 parallel: bool = False,\n",
    "                 **kwargs) -> None:\n",
    "        super(AAEncoder, self).__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.historical_steps = historical_steps\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.parallel = parallel\n",
    "\n",
    "        self.center_embed = SingleInputEmbedding(in_channel=node_dim, out_channel=embed_dim)\n",
    "        self.nbr_embed = MultipleInputEmbedding(in_channels=[node_dim, edge_dim], out_channel=embed_dim)\n",
    "        self.lin_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_self = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.lin_ih = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_hh = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout))\n",
    "        self.bos_token = nn.Parameter(torch.Tensor(historical_steps, embed_dim))\n",
    "        nn.init.normal_(self.bos_token, mean=0., std=.02)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                t: Optional[int],\n",
    "                edge_index: Adj,\n",
    "                edge_attr: torch.Tensor,\n",
    "                bos_mask: torch.Tensor,\n",
    "                rotate_mat: Optional[torch.Tensor] = None,\n",
    "                size: Size = None) -> torch.Tensor:\n",
    "        if self.parallel:\n",
    "            if rotate_mat is None:\n",
    "                center_embed = self.center_embed(x.view(self.historical_steps, x.shape[0] // self.historical_steps, -1))\n",
    "            else:\n",
    "                center_embed = self.center_embed(\n",
    "                    torch.matmul(x.view(self.historical_steps, x.shape[0] // self.historical_steps, -1).unsqueeze(-2),\n",
    "                                 rotate_mat.expand(self.historical_steps, *rotate_mat.shape)).squeeze(-2))\n",
    "            center_embed = torch.where(bos_mask.t().unsqueeze(-1),\n",
    "                                       self.bos_token.unsqueeze(-2),\n",
    "                                       center_embed).view(x.shape[0], -1)\n",
    "        else:\n",
    "            if rotate_mat is None:\n",
    "                center_embed = self.center_embed(x)\n",
    "            else:\n",
    "                center_embed = self.center_embed(torch.bmm(x.unsqueeze(-2), rotate_mat).squeeze(-2))\n",
    "            center_embed = torch.where(bos_mask.unsqueeze(-1), self.bos_token[t], center_embed)\n",
    "        center_embed = center_embed + self._mha_block(self.norm1(center_embed), x, edge_index, edge_attr, rotate_mat,\n",
    "                                                      size)\n",
    "        center_embed = center_embed + self._ff_block(self.norm2(center_embed))\n",
    "        return center_embed\n",
    "\n",
    "    def message(self,\n",
    "                edge_index: Adj,\n",
    "                center_embed_i: torch.Tensor,\n",
    "                x_j: torch.Tensor,\n",
    "                edge_attr: torch.Tensor,\n",
    "                rotate_mat: Optional[torch.Tensor],\n",
    "                index: torch.Tensor,\n",
    "                ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> torch.Tensor:\n",
    "        if rotate_mat is None:\n",
    "            nbr_embed = self.nbr_embed([x_j, edge_attr])\n",
    "        else:\n",
    "            if self.parallel:\n",
    "                center_rotate_mat = rotate_mat.repeat(self.historical_steps, 1, 1)[edge_index[1]]\n",
    "            else:\n",
    "                center_rotate_mat = rotate_mat[edge_index[1]]\n",
    "            nbr_embed = self.nbr_embed([torch.bmm(x_j.unsqueeze(-2), center_rotate_mat).squeeze(-2),\n",
    "                                        torch.bmm(edge_attr.unsqueeze(-2), center_rotate_mat).squeeze(-2)])\n",
    "        query = self.lin_q(center_embed_i).view(-1, self.num_heads, self.embed_dim // self.num_heads)\n",
    "        key = self.lin_k(nbr_embed).view(-1, self.num_heads, self.embed_dim // self.num_heads)\n",
    "        value = self.lin_v(nbr_embed).view(-1, self.num_heads, self.embed_dim // self.num_heads)\n",
    "        scale = (self.embed_dim // self.num_heads) ** 0.5\n",
    "        alpha = (query * key).sum(dim=-1) / scale\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = self.attn_drop(alpha)\n",
    "        return value * alpha.unsqueeze(-1)\n",
    "\n",
    "    def update(self,\n",
    "               inputs: torch.Tensor,\n",
    "               center_embed: torch.Tensor) -> torch.Tensor:\n",
    "        inputs = inputs.view(-1, self.embed_dim)\n",
    "        gate = torch.sigmoid(self.lin_ih(inputs) + self.lin_hh(center_embed))\n",
    "        return inputs + gate * (self.lin_self(center_embed) - inputs)\n",
    "\n",
    "    def _mha_block(self,\n",
    "                   center_embed: torch.Tensor,\n",
    "                   x: torch.Tensor,\n",
    "                   edge_index: Adj,\n",
    "                   edge_attr: torch.Tensor,\n",
    "                   rotate_mat: Optional[torch.Tensor],\n",
    "                   size: Size) -> torch.Tensor:\n",
    "        center_embed = self.out_proj(self.propagate(edge_index=edge_index, x=x, center_embed=center_embed,\n",
    "                                                    edge_attr=edge_attr, rotate_mat=rotate_mat, size=size))\n",
    "        return self.proj_drop(center_embed)\n",
    "\n",
    "    def _ff_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class TemporalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 historical_steps: int,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 num_layers: int = 4,\n",
    "                 dropout: float = 0.1) -> None:\n",
    "        super(TemporalEncoder, self).__init__()\n",
    "        encoder_layer = TemporalEncoderLayer(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers,\n",
    "                                                         norm=nn.LayerNorm(embed_dim))\n",
    "        self.padding_token = nn.Parameter(torch.Tensor(historical_steps, 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.Tensor(historical_steps + 1, 1, embed_dim))\n",
    "        attn_mask = self.generate_square_subsequent_mask(historical_steps + 1)\n",
    "        self.register_buffer('attn_mask', attn_mask)\n",
    "        nn.init.normal_(self.padding_token, mean=0., std=.02)\n",
    "        nn.init.normal_(self.cls_token, mean=0., std=.02)\n",
    "        nn.init.normal_(self.pos_embed, mean=0., std=.02)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.where(padding_mask.t().unsqueeze(-1), self.padding_token, x)\n",
    "        expand_cls_token = self.cls_token.expand(-1, x.shape[1], -1)\n",
    "        x = torch.cat((x, expand_cls_token), dim=0)\n",
    "        x = x + self.pos_embed\n",
    "        out = self.transformer_encoder(src=x, mask=self.attn_mask, src_key_padding_mask=None)\n",
    "        return out[-1]  # [N, D]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(seq_len: int) -> torch.Tensor:\n",
    "        mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "class TemporalEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 dropout: float = 0.1) -> None:\n",
    "        super(TemporalEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim * 4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(embed_dim * 4, embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = src\n",
    "        x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
    "        x = x + self._ff_block(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def _sa_block(self,\n",
    "                  x: torch.Tensor,\n",
    "                  attn_mask: Optional[torch.Tensor],\n",
    "                  key_padding_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _ff_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(F.relu_(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class ALEncoder(MessagePassing):\n",
    "\n",
    "    def __init__(self,\n",
    "                 node_dim: int,\n",
    "                 edge_dim: int,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 dropout: float = 0.1,\n",
    "                 **kwargs) -> None:\n",
    "        super(ALEncoder, self).__init__(aggr='add', node_dim=0, **kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.lane_embed = MultipleInputEmbedding(in_channels=[node_dim, edge_dim], out_channel=embed_dim)\n",
    "        self.lin_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_self = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.lin_ih = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lin_hh = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout))\n",
    "        self.is_intersection_embed = nn.Parameter(torch.Tensor(2, embed_dim))\n",
    "        self.turn_direction_embed = nn.Parameter(torch.Tensor(3, embed_dim))\n",
    "        self.traffic_control_embed = nn.Parameter(torch.Tensor(2, embed_dim))\n",
    "        nn.init.normal_(self.is_intersection_embed, mean=0., std=.02)\n",
    "        nn.init.normal_(self.turn_direction_embed, mean=0., std=.02)\n",
    "        nn.init.normal_(self.traffic_control_embed, mean=0., std=.02)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                x: Tuple[torch.Tensor, torch.Tensor],\n",
    "                edge_index: Adj,\n",
    "                edge_attr: torch.Tensor,\n",
    "                is_intersections: torch.Tensor,\n",
    "                turn_directions: torch.Tensor,\n",
    "                traffic_controls: torch.Tensor,\n",
    "                rotate_mat: Optional[torch.Tensor] = None,\n",
    "                size: Size = None) -> torch.Tensor:\n",
    "        x_lane, x_actor = x\n",
    "        is_intersections = is_intersections.long()\n",
    "        turn_directions = turn_directions.long()\n",
    "        traffic_controls = traffic_controls.long()\n",
    "        x_actor = x_actor + self._mha_block(self.norm1(x_actor), x_lane, edge_index, edge_attr, is_intersections,\n",
    "                                            turn_directions, traffic_controls, rotate_mat, size)\n",
    "        x_actor = x_actor + self._ff_block(self.norm2(x_actor))\n",
    "        return x_actor\n",
    "\n",
    "    def message(self,\n",
    "                edge_index: Adj,\n",
    "                x_i: torch.Tensor,\n",
    "                x_j: torch.Tensor,\n",
    "                edge_attr: torch.Tensor,\n",
    "                is_intersections_j,\n",
    "                turn_directions_j,\n",
    "                traffic_controls_j,\n",
    "                rotate_mat: Optional[torch.Tensor],\n",
    "                index: torch.Tensor,\n",
    "                ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> torch.Tensor:\n",
    "        if rotate_mat is None:\n",
    "            x_j = self.lane_embed([x_j, edge_attr],\n",
    "                                  [self.is_intersection_embed[is_intersections_j],\n",
    "                                   self.turn_direction_embed[turn_directions_j],\n",
    "                                   self.traffic_control_embed[traffic_controls_j]])\n",
    "        else:\n",
    "            rotate_mat = rotate_mat[edge_index[1]]\n",
    "            x_j = self.lane_embed([torch.bmm(x_j.unsqueeze(-2), rotate_mat).squeeze(-2),\n",
    "                                   torch.bmm(edge_attr.unsqueeze(-2), rotate_mat).squeeze(-2)],\n",
    "                                  [self.is_intersection_embed[is_intersections_j],\n",
    "                                   self.turn_direction_embed[turn_directions_j],\n",
    "                                   self.traffic_control_embed[traffic_controls_j]])\n",
    "        query = self.lin_q(x_i).view(-1, self.num_heads, self.embed_dim // self.num_heads)\n",
    "        key = self.lin_k(x_j).view(-1, self.num_heads, self.embed_dim // self.num_heads)\n",
    "        value = self.lin_v(x_j).view(-1, self.num_heads, self.embed_dim // self.num_heads)\n",
    "        scale = (self.embed_dim // self.num_heads) ** 0.5\n",
    "        alpha = (query * key).sum(dim=-1) / scale\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = self.attn_drop(alpha)\n",
    "        return value * alpha.unsqueeze(-1)\n",
    "\n",
    "    def update(self,\n",
    "               inputs: torch.Tensor,\n",
    "               x: torch.Tensor) -> torch.Tensor:\n",
    "        x_actor = x[1]\n",
    "        inputs = inputs.view(-1, self.embed_dim)\n",
    "        gate = torch.sigmoid(self.lin_ih(inputs) + self.lin_hh(x_actor))\n",
    "        return inputs + gate * (self.lin_self(x_actor) - inputs)\n",
    "\n",
    "    def _mha_block(self,\n",
    "                   x_actor: torch.Tensor,\n",
    "                   x_lane: torch.Tensor,\n",
    "                   edge_index: Adj,\n",
    "                   edge_attr: torch.Tensor,\n",
    "                   is_intersections: torch.Tensor,\n",
    "                   turn_directions: torch.Tensor,\n",
    "                   traffic_controls: torch.Tensor,\n",
    "                   rotate_mat: Optional[torch.Tensor],\n",
    "                   size: Size) -> torch.Tensor:\n",
    "        x_actor = self.out_proj(self.propagate(edge_index=edge_index, x=(x_lane, x_actor), edge_attr=edge_attr,\n",
    "                                               is_intersections=is_intersections, turn_directions=turn_directions,\n",
    "                                               traffic_controls=traffic_controls, rotate_mat=rotate_mat, size=size))\n",
    "        return self.proj_drop(x_actor)\n",
    "\n",
    "    def _ff_block(self, x_actor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x_actor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_encoder = LocalEncoder(historical_steps=historical_steps,\n",
    "                                          node_dim=node_dim,\n",
    "                                          edge_dim=edge_dim,\n",
    "                                          embed_dim=embed_dim,\n",
    "                                          num_heads=num_heads,\n",
    "                                          dropout=dropout,\n",
    "                                          num_temporal_layers=num_temporal_layers,\n",
    "                                          local_radius=local_radius,\n",
    "                                          parallel=parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalEncoder(\n",
       "  (aa_encoder): AAEncoder()\n",
       "  (temporal_encoder): TemporalEncoder(\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TemporalEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TemporalEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TemporalEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TemporalEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (al_encoder): ALEncoder()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rotate:\n",
    "    rotate_mat = torch.empty(data.num_nodes, 2, 2, device='cpu')\n",
    "    sin_vals = torch.sin(data['rotate_angles'])\n",
    "    cos_vals = torch.cos(data['rotate_angles'])\n",
    "    rotate_mat[:, 0, 0] = cos_vals\n",
    "    rotate_mat[:, 0, 1] = -sin_vals\n",
    "    rotate_mat[:, 1, 0] = sin_vals\n",
    "    rotate_mat[:, 1, 1] = cos_vals\n",
    "    if data.y is not None:\n",
    "        data.y = torch.bmm(data.y, rotate_mat)\n",
    "    data['rotate_mat'] = rotate_mat\n",
    "else:\n",
    "    data['rotate_mat'] = None\n",
    "\n",
    "local_embed = local_encoder(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CarlaDataBatch(x=[24, 50, 2], y=[24, 50, 2], positions=[24, 100, 2], seq_id=[1], num_nodes=24, padding_mask=[24, 100], bos_mask=[24, 50], rotate_angles=[24], lane_vectors=[812, 2], lane_ctrs=[812, 2], lane_idcs=[812], theta=[1], rotate_mat=[24, 2, 2], obj_type_02=[1], obj_type_04=[1], obj_type_06=[1], obj_type_08=[1], in_av_range=[1], origin=[1, 2], batch=[24], ptr=[2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset=~data['padding_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
